{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "import math\n",
    "import operator\n",
    "import pygeohash as pgh\n",
    "import boto3\n",
    "import re\n",
    "import calendar\n",
    "\n",
    "from datetime import (datetime, date, time, timedelta)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import (when, udf, col, lit, lead, lag, row_number)\n",
    "from pyspark.sql import (SparkSession, DataFrameReader)\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType, DateType\n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "#from pyspark.sql.functions import col,array_contains\n",
    "from pyspark.sql import functions as py\n",
    "from sys import exit\n",
    "from tabulate import tabulate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpe = \"2800M\"\n",
    "pyspark_submit_args = ' --executor-memory ' + mpe + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "master_location = \"spark://172.25.0.09:7077\"\n",
    "confg = pyspark.SparkConf()\n",
    "confg.setMaster(master_location)\n",
    "context = pyspark.SparkContext(conf=confg)        \n",
    "sqlContext = SQLContext(context)\n",
    "session = SparkSession.builder.appName('clean_and_cluster').getOrCreate()\n",
    "dataFrameReader = session.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(lat1, lon1, lat2, lon2, unit='miles'):\n",
    "    '''\n",
    "    Measure simple haversine distance between two points. Default unit = miles.\n",
    "    '''    \n",
    "    units = {'miles':3963.19,'kilometers':6378.137,'meters':6378137,'feet':20902464}    \n",
    "\n",
    "    phi_1 = py.radians(lat1)\n",
    "    phi_2 = py.radians(lat2)\n",
    "    delta_phi = py.radians(lat2 - lat1)\n",
    "    delta_lambda = py.radians(lon2-lon1)\n",
    "\n",
    "    area = py.sin(delta_phi/2.0) ** 2 \\\n",
    "    + py.cos(phi_1) * py.cos(phi_2) * \\\n",
    "    py.sin(delta_lambda / 2.0) ** 2\n",
    "\n",
    "    central_angle = 2 * py.asin((area ** 0.5))\n",
    "    radius = units[unit.lower()]\n",
    "\n",
    "    return py.abs(py.round((central_angle * radius),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_dr(lat1, lon1, lat2, lon2, unit='miles'):\n",
    "    units = {'miles':3963.19,'kilometers':6378.137,'meters':6378137,'feet':20902464}\n",
    "\n",
    "\n",
    "    phi_1 = math.radians(lat1)\n",
    "    phi_2 = math.radians(lat2)\n",
    "    delta_phi = math.radians(lat2 - lat1)\n",
    "    delta_lambda = math.radians(lon2-lon1)\n",
    "\n",
    "    area = math.sin(delta_phi/2.0) ** 2 \\\n",
    "    + math.cos(phi_1) * math.cos(phi_2) * \\\n",
    "    math.sin(delta_lambda / 2.0) ** 2\n",
    "\n",
    "    central_angle = 2 * math.asin((area ** 0.5))\n",
    "    radius = units[unit.lower()]\n",
    "\n",
    "    return abs(round((central_angle * radius),4))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_of_speed(distance, seconds, time_unit='hour'):\n",
    "    '''\n",
    "    Returns the rate of speed based on input distance and seconds\n",
    "    '''\n",
    "    time = {'second':1, 'minute':60, 'hour':3600, 'day':86400, 'year':31536000}\n",
    "    return py.abs(distance / (seconds / time[time_unit]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_dates(object='daily-feed'):\n",
    "    '''\n",
    "    Returns a list of GPS dates present in S3\n",
    "    Options include: daily-feed, micro-clusters, home, work\n",
    "    '''\n",
    "    \n",
    "    client = boto3.client('s3')\n",
    "    paginator = client.get_paginator('list_objects')\n",
    "    gps_bucket = 'b6-8f-fc-09-0f-db-50-3f-gpsdata'\n",
    "    \n",
    "    if object == 'daily-feed':\n",
    "        gps_prefix = 'cuebiq/daily-feed/US/'  \n",
    "        foo_bar = 123456789\n",
    "    elif object == 'micro-clusters':\n",
    "        gps_prefix = 'cuebiq/processed-data/US/micro-clusters/'\n",
    "    elif object == 'home':\n",
    "        gps_prefix = 'cuebiq/processed-data/US/home/20'\n",
    "    elif object == 'work':\n",
    "        gps_prefix = 'cuebiq/processed-data/US/work/20'\n",
    "    else:\n",
    "        return\n",
    "\n",
    "    dates = set()\n",
    "    for result in paginator.paginate(Bucket=gps_bucket, Prefix=gps_prefix, Delimiter='/'):\n",
    "        for prefix in result.get('CommonPrefixes'):\n",
    "            dates.add(datetime.strptime((re.search(\"(\\d{6,})\", prefix.get('Prefix')).group() + '01')[0:8], '%Y%m%d'))\n",
    "    return sorted(list(dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2020, 8, 9, 0, 0), datetime.datetime(2020, 8, 10, 0, 0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_days = s3_dates('daily-feed')\n",
    "micro_days = s3_dates('micro-clusters')\n",
    "\n",
    "study_dts = sorted(list(set(df_days) - set(micro_days) - set([max(df_days)])))\n",
    "study_dts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2020, 7, 30, 0, 0),\n",
       " datetime.datetime(2020, 7, 29, 0, 0),\n",
       " datetime.datetime(2020, 8, 1, 0, 0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use fake date values for dev\n",
    "\n",
    "#study_dts = study_dates()\n",
    "\n",
    "study_dts = [datetime.strptime('2020-07-30', '%Y-%m-%d'),datetime.strptime('2020-07-29', '%Y-%m-%d'), datetime.strptime('2020-08-01', '%Y-%m-%d')]\n",
    "\n",
    "#home_months = [datetime.strptime('2020-07-01', '%Y-%m-%d')]\n",
    "#work_months = [datetime.strptime('2020-07-01', '%Y-%m-%d')\n",
    "\n",
    "study_dts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create universe of all dates to pull from S3 (rolling three days per study date)\n",
    "dt_universe = set()\n",
    "for dt in study_dts:\n",
    "    dt_universe.add(dt + timedelta(days=-1))\n",
    "    dt_universe.add(dt)\n",
    "    dt_universe.add(dt + timedelta(days=1))\n",
    "    \n",
    "dt_universe\n",
    "len(set(dt_universe) - set(df_days))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exit script if a date that needs to be pulled is unaccounted for in daily-feed\n",
    "if len(set(dt_universe) - set(df_days)) > 0:\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1596081600, 1595995200, 1596254400]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply daily time offset (in minutes)\n",
    "offset = 240\n",
    "for index,dt in enumerate(study_dts):\n",
    "    study_dts[index] = dt + timedelta(minutes=offset)\n",
    "\n",
    "unix_dts = []\n",
    "for dt in study_dts:\n",
    "    unix_dts.append(calendar.timegm(dt.timetuple()))\n",
    "unix_dts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/opt/spark/sample_data/daily-feed/US/20200731*/*.csv.gz',\n",
       " '/opt/spark/sample_data/daily-feed/US/20200728*/*.csv.gz',\n",
       " '/opt/spark/sample_data/daily-feed/US/20200802*/*.csv.gz',\n",
       " '/opt/spark/sample_data/daily-feed/US/20200729*/*.csv.gz',\n",
       " '/opt/spark/sample_data/daily-feed/US/20200730*/*.csv.gz',\n",
       " '/opt/spark/sample_data/daily-feed/US/20200801*/*.csv.gz']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converted universe dates into S3 object paths\n",
    "\n",
    "#base_path = 's3://b6-8f-fc-09-0f-db-50-3f-gpsdata/cuebiq/daily-feed/US/'\n",
    "base_path = \"/opt/spark/sample_data/daily-feed/US/\"\n",
    "dt_paths = []\n",
    "for dt in dt_universe:\n",
    "    dt_paths.append(base_path + str(dt).replace('-','')[0:8] + '*/*.csv.gz')\n",
    "\n",
    "dt_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"utc_timestamp\",IntegerType(),True),\n",
    "                        StructField(\"device_id\",StringType(),True),\n",
    "                        StructField(\"os\",IntegerType(),True),\n",
    "                        StructField(\"latitude\",FloatType(),True),\n",
    "                        StructField(\"longitude\",FloatType(),True),\n",
    "                        StructField(\"accuracy\",IntegerType(),True),\n",
    "                        StructField(\"tz_offset\",IntegerType(),True)])\n",
    "\n",
    "def read_csv(schema, paths):   \n",
    "    df = sqlContext.read \\\n",
    "        .option(\"sep\", '\\t') \\\n",
    "        .option(\"inferSchema\", False) \\\n",
    "        .option(\"header\", False) \\\n",
    "        .schema(schema) \\\n",
    "        .csv(paths.split(','))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the following dates:\n",
      "   07/29/20\n",
      "   07/30/20\n",
      "   08/01/20\n",
      "Phase             Remaining Pings    Removed Pings    Percent Reduction  Description\n",
      "--------------  -----------------  ---------------  -------------------  --------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Starting count            4072669                0                 0.00  Count of pings before filtering\n",
      "Null values               4072669                0                 0.00  Empty values among latitude, longitude, accuracy, timestamp, tz offset\n",
      "Duplicates                3819596           253073                 6.21  Records with duplicate device_id and utc_timestamps\n",
      "Accuracy                  2562796          1256800                30.86  Horizontal accuracy values exceeding safe thresholds (outside of 5 - 65)\n",
      "Coordinates               2562647              149                 0.00  Pings occurring outside a general rectangle covering the US (including AK, HI)\n",
      "Study date(s)             1263325          1299322                31.90  Pings occuring outside the study date range when tz_offset is applied. Process brings in preceding and following dates from GPS provider to capture straggling pings\n",
      "Final count               1263325          2809344                68.98  Final pings after junk filtering process\n"
     ]
    }
   ],
   "source": [
    "print(\"Running the following dates:\")\n",
    "for dt in sorted(study_dts):\n",
    "    print(\"   \" + dt.strftime('%x'))\n",
    "\n",
    "\n",
    "# Populate dataframe by sending in comma-separated file paths\n",
    "df = read_csv(schema, \",\".join(dt_paths))\n",
    "init_cnt = df.cache().count()\n",
    "\n",
    "# Drop records with null values in critical fields\n",
    "df = df.na.drop(subset=['latitude','longitude','utc_timestamp','tz_offset','accuracy'])\n",
    "null_cnt = df.cache().count()\n",
    "\n",
    "# Drop duplicate records based on device_id and timestamp\n",
    "df = df.dropDuplicates(['utc_timestamp','device_id'])\n",
    "dupe_cnt = df.cache().count()\n",
    "                   \n",
    "# Remove records falling outside safe horizontal accuracy thresholds\n",
    "df = df.filter((df['accuracy'] >= 5) & (df['accuracy'] <= 65))\n",
    "acc_cnt = df.cache().count()\n",
    "\n",
    "# Remove records falling outside of a bounding rectanlge of the US (not just contintental)\n",
    "df = df.filter((df['latitude'] >= 18) & (df['latitude'] < 72) & ((df['longitude'] > 171) | (df['longitude'] < -63)))\n",
    "coord_cnt = df.cache().count()\n",
    "\n",
    "# Remove records falling outside of the study range scope\n",
    "schema = StructType([StructField(\"utc_timestamp\",IntegerType(),True),\n",
    "                        StructField(\"device_id\",StringType(),True),\n",
    "                        StructField(\"os\",IntegerType(),True),\n",
    "                        StructField(\"latitude\",FloatType(),True),\n",
    "                        StructField(\"longitude\",FloatType(),True),\n",
    "                        StructField(\"accuracy\",IntegerType(),True),\n",
    "                        StructField(\"tz_offset\",IntegerType(),True),\n",
    "                        StructField(\"study_dt\", StringType(),True)])   \n",
    "\n",
    "fnl_df = sqlContext.createDataFrame(context.emptyRDD(),schema) \n",
    "\n",
    "for dt in unix_dts:\n",
    "    fnl_df = fnl_df.union(df.filter((df['utc_timestamp'] + df['tz_offset']).between(dt, dt + 86399)) \\\n",
    "                          .withColumn(\"study_dt\",py.to_date(py.from_unixtime(lit(dt)))))\n",
    "\n",
    "tm_cnt = fnl_df.cache().count()\n",
    "fnl_df = fnl_df.repartition(8, 'device_id')\n",
    "\n",
    "tbl_data = [['Starting count', init_cnt, 0, 0, 'Count of pings before filtering'], \\\n",
    "           ['Null values', null_cnt, init_cnt - null_cnt, ((init_cnt - null_cnt) / float(init_cnt)) * 100, 'Empty values among latitude, longitude, accuracy, timestamp, tz offset'], \\\n",
    "           ['Duplicates', dupe_cnt, null_cnt - dupe_cnt, ((null_cnt - dupe_cnt) / float(init_cnt)) * 100, 'Records with duplicate device_id and utc_timestamps'], \\\n",
    "           ['Accuracy', acc_cnt, dupe_cnt - acc_cnt, ((dupe_cnt - acc_cnt) / float(init_cnt)) * 100, 'Horizontal accuracy values exceeding safe thresholds (outside of 5 - 65)'], \\\n",
    "           ['Coordinates', coord_cnt, acc_cnt - coord_cnt, ((acc_cnt - coord_cnt) / float(init_cnt)) * 100, 'Pings occurring outside a general rectangle covering the US (including AK, HI)'], \\\n",
    "           ['Study date(s)', tm_cnt, coord_cnt - tm_cnt, ((coord_cnt - tm_cnt) / float(init_cnt)) * 100, 'Pings occuring outside the study date range when tz_offset is applied. Process brings in preceding and following dates from GPS provider to capture straggling pings'], \\\n",
    "           ['Final count', tm_cnt, init_cnt - tm_cnt, ((init_cnt - tm_cnt) / float(init_cnt)) * 100, 'Final pings after junk filtering process']]\n",
    "\n",
    "print(tabulate(tbl_data, floatfmt=\".2f\", headers=['Phase', 'Remaining Pings', 'Removed Pings', 'Percent Reduction', 'Description']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fnl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ping count = 1,263,325\n"
     ]
    }
   ],
   "source": [
    "init_cnt = df.cache().count()\n",
    "print(\"Starting ping count = {:,}\".format(init_cnt))\n",
    "w = Window().partitionBy('device_id', 'study_dt').orderBy('utc_timestamp')\n",
    "\n",
    "df = df.withColumn('dist_to',distance(df['latitude'], df['longitude'], lead(df['latitude'],1).over(w), \\\n",
    "                    lead(df['longitude'],1).over(w))) \\\n",
    "    .withColumn('sec_to', (lead(df['utc_timestamp'], 1).over(w) - df['utc_timestamp'])) \\\n",
    "    .withColumn('speed_to', rate_of_speed(col('dist_to'), col('sec_to'),'hour')) \\\n",
    "    .withColumn('dist_from', lag(col('dist_to'), 1).over(w)) \\\n",
    "    .withColumn('sec_from', lag(col('sec_to'), 1).over(w)) \\\n",
    "    .withColumn('speed_from', lag(col('speed_to'), 1).over(w)) \\\n",
    "    .withColumn('speed_flag', when(((col('dist_to').isNull()) | (col('dist_from').isNull())) \\\n",
    "                 | ((((col('speed_from') + col('speed_to')) / 2) <= 90) | ((col('dist_to') >= 150) | (col('dist_from') >= 150))) \\\n",
    "                 & ((col('speed_from') < 600) & (col('speed_to') < 600)) \\\n",
    "                 & ((col('speed_from') < 30) | (col('speed_to') < 30)), 1).otherwise(0))\n",
    "\n",
    "speed_df = df.filter(df['speed_flag'] == 0) \\\n",
    "                .withColumn('avg_speed', (df['speed_from'] + df['speed_to']) / 2)\n",
    "\n",
    "df = df.filter(df['speed_flag'] == 1) \\\n",
    "        .drop('dist_to', 'dist_from', 'sec_to', 'speed_to', 'sec_from', 'speed_from', 'speed_flag')\n",
    "\n",
    "min_speed = speed_df.agg(py.min(col('avg_speed'))).collect()[0][0]\n",
    "max_speed = speed_df.agg(py.max(col('avg_speed'))).collect()[0][0]\n",
    "\n",
    "sum_df = speed_df.withColumn('speed_bucket', when(col('avg_speed') < 50, str(min_speed) + \" - 49.9\") \\\n",
    "                            .when((col('avg_speed') >= 50) & (col('avg_speed') < 60), \"50 - 59.9\") \\\n",
    "                            .when((col('avg_speed') >= 60) & (col('avg_speed') < 70), \"60 - 69.9\") \\\n",
    "                             .when((col('avg_speed') >= 70) & (col('avg_speed') < 80), \"70 - 79.9\") \\\n",
    "                             .when((col('avg_speed') >= 80) & (col('avg_speed') < 90), \"80 - 89.9\") \\\n",
    "                             .when((col('avg_speed') >= 90) & (col('avg_speed') < 100), \"90 - 99.9\") \\\n",
    "                             .when(col('avg_speed') >= 100, \"100 - \" + str(max_speed)) \\\n",
    "                             .otherwise(lit(\"other\")))\n",
    "\n",
    "speed_tbl = sum_df.groupBy(sum_df['speed_bucket']).count().collect()\n",
    "\n",
    "\n",
    "tab_row = []\n",
    "for row in speed_tbl:\n",
    "    tab_row.append([float(re.search(\"(\\S+)\", row[0]).group()), \\\n",
    "                    float(re.search(\"[-]\\s(\\S+)\", row[0]).group(1)), \\\n",
    "                   row[1]])\n",
    "    \n",
    "print(tabulate(sorted(tab_row), headers=['Minimum Average (mph)', 'Maximum Average (mph)', 'Ping Count']))\n",
    "\n",
    "#test_df.show(15)\n",
    "#spd_cnt = df.cache().count()\n",
    "#print(\"{:,} pings removed {:.1%}. Ping universe now {:,}\".format(init_cnt - spd_cnt, (init_cnt - spd_cnt) / float(init_cnt), spd_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Minimum Average (mph)    Maximum Average (mph)    Ping Count\n",
      "-----------------------  -----------------------  ------------\n",
      "                  30.03                     49.9        141533\n",
      "                  50                        59.9        114037\n",
      "                  60                        69.9        193964\n",
      "                  70                        79.9        166916\n",
      "                  80                        89.9         32517\n",
      "                  90                        99.9          6917\n",
      "                 100                     24283.8          8092\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ping count = 599,349\n",
      "6,144 pings removed (1.0% of universe). Final ping count = 593,205\n"
     ]
    }
   ],
   "source": [
    "init_cnt = df.cache().count()\n",
    "print(\"Starting ping count = {:,}\".format(init_cnt))\n",
    "\n",
    "# Linear travel filter\n",
    "w = Window().partitionBy('device_id', 'study_dt').orderBy('utc_timestamp')\n",
    "l = Window().partitionBy('device_id', 'study_dt', 'lin_grp').orderBy('utc_timestamp')\n",
    "lgrp = 4\n",
    "\n",
    "df = df.withColumn('RecordNum',row_number().over(w)) \\\n",
    "    .withColumn('lin_grp', py.ceil(row_number().over(w) / lgrp)) \\\n",
    "    .withColumn('dist_to', distance(df['latitude'], df['longitude'], \\\n",
    "          lead(df['latitude'],1).over(l), lead(df['longitude'],1).over(l),'meters')) \\\n",
    "    .withColumn('sequence', row_number().over(l))\n",
    "\n",
    "\n",
    "# Create aggregated table for linear groupings\n",
    "expr = [py.min(col('utc_timestamp')).alias('min_utc_timestamp'),py.max(col('utc_timestamp')).alias('max_utc_timestamp'), \\\n",
    "    py.count(col('utc_timestamp')).alias('cnt'),py.sum(col('dist_to')).alias('sum_dist'),py.min(col('dist_to')).alias('min_dist')]\n",
    "\n",
    "df_grp = df.groupBy('device_id','lin_grp').agg(*expr) \n",
    "\n",
    "df_l = df.filter(df['sequence'].isin([1,lgrp])).join(df_grp, ['device_id','lin_grp'])\n",
    "\n",
    "\n",
    "#Measure the distance between first and last in each linear grouping and compare to sum distance of all points\n",
    "# Only keep groups that meet criteria for being straight-line\n",
    "df_j = df_l.withColumn('strt_dist', distance(df_l['latitude'],df_l['longitude'], \\\n",
    "            lead(df_l['latitude'],1).over(l), \\\n",
    "            lead(df_l['longitude'],1).over(l), 'meters')) \\\n",
    "        .withColumn('lin', col('strt_dist') / df_l['sum_dist']) \\\n",
    "        .na.drop(subset=['strt_dist']) \\\n",
    "        .filter((df_l['min_dist'] > 0)  \\\n",
    "            & (col('strt_dist').between(150, 2000)) \\\n",
    "            & (df_l['cnt'] == 4) \\\n",
    "            & (col('lin') >= .99825)) \\\n",
    "        .select('device_id','lin_grp', 'lin')\n",
    "\n",
    "# Outer join main dataframe to linears groups to filter non-linear pings \n",
    "df = df.join(df_j, ['device_id','lin_grp'], how='left_outer') \\\n",
    "    .filter(col('lin').isNull()) \\\n",
    "    .drop('lin_grp', 'RecordNum', 'dist_to', 'sequence', 'lin')\n",
    "\n",
    "lin_cnt = df.cache().count()\n",
    "print(\"{:,} pings removed ({:.1%} of universe). Final ping count = {:,}\".format(init_cnt - lin_cnt, (init_cnt - lin_cnt) / float(init_cnt), lin_cnt ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_j.orderBy(col('device_id'), col('utc_timestamp')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "u'path file:/opt/spark/sample_data/daily-feed-reduced/2020073000 already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0ac3dfc0f886>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstudy_dts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdf\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'study_dt'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'device_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortWithinPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'device_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'utc_timestamp'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"00\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/readwriter.pyc\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue)\u001b[0m\n\u001b[1;32m    930\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                        encoding=encoding, emptyValue=emptyValue)\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: u'path file:/opt/spark/sample_data/daily-feed-reduced/2020073000 already exists.;'"
     ]
    }
   ],
   "source": [
    "out_path = \"/opt/spark/sample_data/daily-feed-reduced/\"\n",
    "\n",
    "for dt in study_dts:\n",
    "    df \\\n",
    "    .filter(df['study_dt'] == str(dt)[0:10]) \\\n",
    "    .repartition(5, 'device_id').sortWithinPartitions('device_id','utc_timestamp') \\\n",
    "    .write \\\n",
    "    .csv(out_path + str(dt)[0:10].replace(\"-\",\"\") + \"00\", sep = \",\", compression = \"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
